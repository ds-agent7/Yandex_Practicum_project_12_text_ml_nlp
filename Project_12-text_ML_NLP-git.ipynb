{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project.12  NLP. Проект для «Викишоп»\n",
    "\n",
    "    (yandex_practicum by student@pavel_matushevskiy, Volgograd 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание проекта (без BERT)\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Подготовка библиотек и данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T06:17:38.472848Z",
     "start_time": "2022-07-26T06:17:15.807773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.21.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (61.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Building wheels for collected packages: en_core_web_sm\n",
      "  Building wheel for en_core_web_sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en_core_web_sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=b879fe3b673c3013f95a8c9e54fd1c64255e483d93d465ccbaf385e2f74faf84\n",
      "  Stored in directory: /Users/paulmatus/Library/Caches/pip/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en_core_web_sm\n",
      "Installing collected packages: en_core_web_sm\n",
      "Successfully installed en_core_web_sm-2.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers\n",
    "#!python3 -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T18:11:06.401605Z",
     "start_time": "2022-07-26T18:10:59.165373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2-dicts-uk in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (2.4.1.1.1460299261)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pip in /Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2\n"
     ]
    }
   ],
   "source": [
    "#pip install pymorphy2\n",
    "!pip install -U pymorphy2-dicts-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:13:17.358935Z",
     "start_time": "2023-01-03T10:13:09.045339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Блоки импорта\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from pymystem3 import Mystem\n",
    "#import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "import en_core_web_sm\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,make_scorer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils import shuffle\n",
    " \n",
    "# Определим константы\n",
    "RANDOM_STATE = 42\n",
    "TARGET_NAME = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:13:56.786080Z",
     "start_time": "2023-01-03T10:13:55.896140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подгрузим наш файл с данными:\n",
    "try:\n",
    "    df = pd.read_csv('datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:27.659390Z",
     "start_time": "2023-01-03T10:13:57.614171Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "# Вызовем список \"стоп\" слов\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:27.676758Z",
     "start_time": "2023-01-03T10:14:27.671433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Поскольку лемматизация считается 28 минут и остальные операции тоже не быстрые,\n",
    "# для удобства  сохраним наш обработанный датасет в отдельном файле:\n",
    "\n",
    "PREPARED_CSV = 'prepared-text.csv'\n",
    "\n",
    "# Константа будет определять, использовать подготовленный датасет или собирать все заново\n",
    "USE_PREPARED_CSV = True  #False - если нет собранного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:30.772542Z",
     "start_time": "2023-01-03T10:14:27.678222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_prepared</th>\n",
       "      <th>text_cleared</th>\n",
       "      <th>text_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man try edit war guy constantly remove rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "      <td>real suggestion improvement   wonder section s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                       text_prepared  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i am ...   \n",
       "2  hey man, i am really not trying to edit war. i...   \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                        text_cleared  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  daww he matches this background colour i am se...   \n",
       "2  hey man i am really not trying to edit war it ...   \n",
       "3  more i cannot make any real suggestions on imp...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                         text_lemmas  \n",
       "0  explanation edit username hardcore metallica f...  \n",
       "1  daww match background colour seemingly stuck t...  \n",
       "2  hey man try edit war guy constantly remove rel...  \n",
       "3  real suggestion improvement   wonder section s...  \n",
       "4                    sir hero chance remember page s  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_PREPARED_CSV:\n",
    "    df = pd.read_csv(PREPARED_CSV)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:30.872753Z",
     "start_time": "2023-01-03T10:14:30.774638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   Unnamed: 0     159571 non-null  int64 \n",
      " 1   text           159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   text_prepared  159571 non-null  object\n",
      " 4   text_cleared   159564 non-null  object\n",
      " 5   text_lemmas    159465 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1.2. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примерный алгоритм работы\n",
    "\n",
    "    1. Проверим баланс классов\n",
    "    2. Изучим дополнительные параметры текста\n",
    "    3. Соберем леммы из нашего текста\n",
    "    4. Очистим текст от лишних символов и приведем все к нижнему регистру для унификации\n",
    "    5. Подумаем над метапризнаками для лучшего обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.2.1. Проверим дисбаланс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:30.973079Z",
     "start_time": "2023-01-03T10:14:30.873965Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверим дисбаланс\n",
    "df[TARGET_NAME].value_counts().plot(kind='bar', figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика - присутствует сильный дисбаланс классов, и мы учтем это при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:31.107130Z",
     "start_time": "2023-01-03T10:14:30.975140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самый короткий текст комментария: 6\n",
      "самый длинный текст комментария: 5000\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на параметры текста:\n",
    "# найдем самый короткий и самый длинный комментарий:\n",
    "min_size_all = df['text'].str.len().min()\n",
    "max_size_all = df['text'].str.len().max()\n",
    "print(\"самый короткий текст комментария:\", min_size_all)\n",
    "print(\"самый длинный текст комментария:\", max_size_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:31.728899Z",
     "start_time": "2023-01-03T10:14:31.108466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0    159312\n",
      "1       259\n",
      "Name: is_rus_text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что у нас действительно только английский текст, и нет русского:\n",
    "rus_reg = r'[а-яА-ЯёЁ]'\n",
    "eng_reg = r'[A-Za-z]'\n",
    "\n",
    "\n",
    "def find_russian(value):\n",
    "    return int(re.search(rus_reg, value) is not None)\n",
    "\n",
    "\n",
    "df['is_rus_text'] = df['text'].apply(find_russian)\n",
    "\n",
    "print(df['is_rus_text'].unique())\n",
    "print(df['is_rus_text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:31.734516Z",
     "start_time": "2023-01-03T10:14:31.730316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "159566    0\n",
       "159567    0\n",
       "159568    0\n",
       "159569    0\n",
       "159570    0\n",
       "Name: is_rus_text, Length: 159571, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_rus_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:14:31.930117Z",
     "start_time": "2023-01-03T10:14:31.736063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126     Ahh, Hello Witzeman \\n\\n203.92.84.161  \\nSymbols: ~ | ¡ ¿ † ‡ ↔ ↑ ↓ • ¶   # ½ ⅓ ⅔ ¼ ¾ ⅛ ⅜ ⅝ ⅞ ∞   ‘ “ ’ ” «»   ¤ ₳ ฿ ₵ ¢ ₡ ₢ $ ₫ ₯ € ₠ ₣ ƒ ₴ ₭ ₤ ℳ ₥ ₦ № ₧ ₰ £ ៛ ₨ ₪ ৳ ₮ ₩ ¥   ♠ ♣ ♥ ♦   m² m³ \\nCharacters: Á á Ć ć É é Í í Ĺ ĺ Ń ń Ó ó Ŕ ŕ Ś ś Ú ú Ý ý Ź ź   À à È è Ì ì Ò ò Ù ù   Â â Ĉ ĉ Ê ê Ĝ ĝ Ĥ ĥ Î î Ĵ ĵ Ô ô Ŝ ŝ Û û Ŵ ŵ Ŷ ŷ   Ä ä Ë ë Ï ï Ö ö Ü ü Ÿ ÿ   ß   Ã ã Ẽ ẽ Ĩ ĩ Ñ ñ Õ õ Ũ ũ Ỹ ỹ   Ç ç Ģ ģ Ķ ķ Ļ ļ Ņ ņ Ŗ ŗ Ş ş Ţ ţ   Đ đ   Ů ů   Ǎ ǎ Č č Ď ď Ě ě Ǐ ǐ Ľ ľ Ň ň Ǒ ǒ Ř ř Š š Ť ť Ǔ ǔ Ž ž   Ā ā Ē ē Ī ī Ō ō Ū ū Ȳ ȳ Ǣ ǣ   ǖ ǘ ǚ ǜ Ă ă Ĕ ĕ Ğ ğ Ĭ ĭ Ŏ ŏ Ŭ ŭ   Ċ ċ Ė ė Ġ ġ İ ı Ż ż   Ą ą Ę ę Į į Ǫ ǫ Ų ų   Ḍ ḍ Ḥ ḥ Ḷ ḷ Ḹ ḹ Ṃ ṃ Ṇ ṇ Ṛ ṛ Ṝ ṝ Ṣ ṣ Ṭ ṭ   Ł ł   Ő ő Ű ű   Ŀ ŀ   Ħ ħ   Ð ð Þ þ   Œ œ   Æ æ Ø ø Å å   Ə ə    \\nGreek: Ά ά Έ έ Ή ή Ί ί Ό ό Ύ ύ Ώ ώ   Α α Β β Γ γ Δ δ   Ε ε Ζ ζ Η η Θ θ   Ι ι Κ κ Λ λ Μ μ   Ν ν Ξ ξ Ο ο Π π   Ρ ρ Σ σ ς Τ τ Υ υ   Φ φ Χ χ Ψ ψ Ω ω    \\nCyrillic: А а Б б В в Г г   Ґ ґ Ѓ ѓ Д д Ђ ђ   Е е Ё ё Є є Ж ж   З з Ѕ ѕ И и І і   Ї ї Й й Ј ј К к   Ќ ќ Л л Љ љ М м   Н н Њ њ О о П п   Р р С с Т т Ћ ћ   У у Ў ў Ф ф Х х   Ц ц Ч ч Џ џ Ш ш   Щ щ Ъ ъ Ы ы Ь ь   Э э Ю ю Я я \\nIPA: t̪ d̪ ʈ ɖ ɟ ɡ ɢ ʡ ʔ   ɸ ʃ ʒ ɕ ʑ ʂ ʐ ʝ ɣ ʁ ʕ ʜ ʢ ɦ   ɱ ɳ ɲ ŋ ɴ   ʋ ɹ ɻ ɰ   ʙ ʀ ɾ ɽ   ɫ ɬ ɮ ɺ ɭ ʎ ʟ   ɥ ʍ ɧ   ɓ ɗ ʄ ɠ ʛ   ʘ ǀ ǃ ǂ ǁ   ɨ ʉ ɯ   ɪ ʏ ʊ   ɘ ɵ ɤ   ə ɚ   ɛ ɜ ɝ ɞ ʌ ɔ   ɐ ɶ ɑ ɒ   ʰ ʷ ʲ ˠ ˤ ⁿ ˡ   ˈ ˌ ː ˑ ̪   \\n= My Famous Article ==witze  happiness − wikipedia The Witzeman is a great honour that has been passed down through the generations of many families, regardless of race, age, character or knowledge. The outside world knows little about these elusive characters, and the honour of the Witzeman. For those who have ever been a Witzeman, it has been said to have been a great honour, although the qualities needed for the job have never been disclosed. A person may not now they were a Witzeman for many years, until they are called by former Witzemans via dreams. Associates of the Witzeman are known to be a certain Babe Cool or the much lesser known Witzewoman.\\n\\nThe Present Witzeman is an 11-year-old boy namely Benjamin Woods, who is said to have become a Witzeman after he felt this 'awesome radiance in his top-right bumcheek'. He has not told a great number of people his testimony, but has promised to do so in years to come.\\n\\n                           History\\nThe present world has only come enlightned with the knowledge of the Witzeman in present years, because former Witzemen have not been so public about the honour, to abide with the formality and conservativism in their time. The term 'Witzeman' is a compound word of the adjective Witze and 'man', witze being an 11th century term for 'a person of extreme humour and radiant intelligence'. Unfortuneatly, Witze is also sometimes associated with bad wind, for unspeakable reasons.\\n\\n                       Proper History\\nAs early as the 14th century, Witzemen were considered outcasts, and young children were encouraged to poke them with sticks. This led to the 1st Witzeman Rebellion of 1555 when the current Witzeman and his followers attempted a coup to take over the local Council chess team. This rebellion further disgraced the Witzeman, as he and his followers were embarassingly pronounced 'gaga' at a Government meeting that year. The period from here to the early 1900s was a dark time in the Witzemans history. However good times were to come for the Witzeman. Many normal people took part in mass demonstrations and protests for the Witzeman in the 1980s, building up to a great moment in the history...the Great Rebellion of 1988. This was when several Witzeman sprinted round the Visitors Gallery of the House of Commons, wearing clown masks, but otherwise completely naked. This was thought to be an act of circambulation. However many people who witnessed this shocking behaviour, were 'mentally, spiritually, and emotionallly scarred for life'. This group of people included many young children who were forced to go to asylums in a state of mental instability, suffering from trauma. It was at this time that the Government finally took positive action for the Witzeman. In 1990, a bill of rights for the Witzeman was signed, stating all Witzemen were allowed to do as they pleased. Although this was a formal agreement, many journalists viewed it as a letter of submission from the Government to the Witzeman, as the behaviour of the Witzeman and his followers became more and more twisted.\n",
      "228     http://www.users.bigpond.com/MONTDALE/page8.html  Heritage from village Κρανιώνας in macedonian Дреновени. Sources claim that the village was pure Slavic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "449     \"\\n\\n Yobot: incorrect DEFAULTSORT \\n\\nHi, Yobot  an incorrect DEFAULTSORT. It's puzzling as to why it didn't recognize the article as being about a person. Among other clues, it included birth and death dates at the top, as well as a  (which Yobot substituted to additionally include the correct DEFAULTSORT).  • XAЯAbИAM \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "996     It is common knowledge that Karaims (but not Karaite Jews) boast descent from Khazars, but what they do not realize is that we believe the Он-окъ (On-oq) whence came (mixing with Carian mercenaries from Caphtor and Kolhkis) our Khazar ancestors were in fact the remnant of the lost Ten-Tribes of Israel who formed our priesthood. It is precisely for this reason that Karaite Jewish scholars like Jacob Ben Reuben of Byzanteum and Yefet ben Ali or Jeshua ben Judah call us bastards in their writings. \\n\\nThe following editor (                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "1907    Hi, 牛岩, and welcome to Wikipedia! I hope you like the place and decide to stay. Here are some pages that you might find helpful:\\n\\nIntroduction to Wikipedia\\nThe five pillars of Wikipedia\\nHow to edit a page and How to develop articles\\nHow to create your first article \\nSimplified Manual of Style\\n\\nPlease remember to sign your messages on talk pages by typing four tildes (~~~~); this will automatically insert your username and the date. If you need help, check out Wikipedia:Questions, ask me on my talk page, or ask your question on this page and then place {{help me}} before the question.\\n\\nHowever, there is a small issue with your username. As it is in a non-Latin alphabet script it may not display correctly for many other users. Although usernames like yours are not prohibited, Wikipedia's  signature guideline and username policy encourage you, as a courtesy to other users, to alter your signature so that it also includes a transliteration of your username using Latin characters, so others can see it correctly. For the how-to of tailoring your username, please see WP:CUSTOMSIG.\\n\\nThanks, and feel free to message me about anything.\\n\\nترجمة |\\nթարգմանություն |\\ntərcümə |\\nпераклад |\\nঅনুবাদ |\\nпревод |\\n翻译 |\\nთარგმანი |\\nμετάφραση |\\nઅનુવાદ |\\nתרגום |\\nअनुवाद |\\n翻訳 |\\nಅನುವಾದ |\\nបកប្រែ |\\n번역 |\\nການແປພາສາ |\\nпревод |\\nभाषांतर करणे] |\\nترجمه |\\nперевод |\\nпревод |\\nமொழிபெயர்ப்பு |\\nఅనువాద |\\nการแปล |\\nпереклад |\\ndịch |\\n |\\nיבערזעצונג                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/n21wvl254wj5bjdrpk0q3z000000gn/T/ipykernel_1177/423488444.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на содержание \"русских\" комментариев:\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(df[df['is_rus_text'] == 1]['text'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Промежуточные выводы:   \n",
    "\n",
    "\n",
    "        В итоге у нас 259 записи-комментарии, в которых присутствуют символы\\отдельные слова кирилицы. Большого влияния на нашу работу думаю они не окажут. Оставим их.\n",
    "\n",
    "    Далее произведем обработку\\очистку текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:17.229639Z",
     "start_time": "2023-01-03T10:19:13.423169Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_russian(text):\n",
    "    return re.sub(r'[^A-Za-z]', ' ', text)\n",
    "\n",
    "df['text'] = df['text'].apply(delete_russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:17.938457Z",
     "start_time": "2023-01-03T10:19:17.929671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вернем первоначальные настройки ширины текста и удалим ненужные данные:\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "del df['is_rus_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:18.678792Z",
     "start_time": "2023-01-03T10:19:18.646579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  More I can t make any real suggestions on improvement   I wondered if the section statistics should be later on  or a subsection of   types of accidents     I think the references may need tidying so that they are all in the exact same format ie date format etc  I can do that later on  if no one else does first   if you have any preferences for formatting style on references or want to do it yourself please let me know   There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up  It s listed in the relevant form eg Wikipedia Good article nominations Transport   '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:20.541571Z",
     "start_time": "2023-01-03T10:19:20.518011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Создадим столбец-признак для обработанного текста,\n",
    "# сразу приведем к нижнему регистру, а также обработаем частые сокращения:\n",
    "if not USE_PREPARED_CSV:\n",
    "    df['text_prepared'] = df['text'].str.lower()\n",
    "    reduction_dict = dict()\n",
    "    reduction_dict[\"i'm\"] = 'i am'\n",
    "    reduction_dict[\"i'd\"] = 'i had'\n",
    "    reduction_dict[\"i'll\"] = 'i will'\n",
    "    reduction_dict[\"i've\"] = 'i have'\n",
    "    reduction_dict[\"you're\"] = 'you are'\n",
    "    reduction_dict[\"you'd\"] = 'you had'\n",
    "    reduction_dict[\"you'll\"] = 'you will'\n",
    "    reduction_dict[\"you've\"] = 'you have'\n",
    "    reduction_dict[\"he's\"] = 'he is'\n",
    "    reduction_dict[\"he'd\"] = 'he had'\n",
    "    reduction_dict[\"he'll\"] = 'he will'\n",
    "    reduction_dict[\"she's\"] = 'she is'\n",
    "    reduction_dict[\"she'd\"] = 'she had'\n",
    "    reduction_dict[\"she'll\"] = 'she will'\n",
    "    reduction_dict[\"it's\"] = 'it is'\n",
    "    reduction_dict[\"it'll\"] = 'it will'\n",
    "    reduction_dict[\"we're\"] = 'we are'\n",
    "    reduction_dict[\"we'd\"] = 'we would'\n",
    "    reduction_dict[\"we'll\"] = 'we will'\n",
    "    reduction_dict[\"we've\"] = 'we have'\n",
    "    reduction_dict[\"they're\"] = 'they are'\n",
    "    reduction_dict[\"they'd\"] = 'they had'\n",
    "    reduction_dict[\"they'll\"] = 'they will'\n",
    "    reduction_dict[\"they've\"] = 'they have'\n",
    "    reduction_dict[\"there's\"] = 'there is'\n",
    "    reduction_dict[\"there'll\"] = 'there will'\n",
    "    reduction_dict[\"there'd\"] = 'there would'\n",
    "    reduction_dict[\"isn't\"] = 'is not'\n",
    "    reduction_dict[\"aren't\"] = 'are not'\n",
    "    reduction_dict[\"don't\"] = 'do not'\n",
    "    reduction_dict[\"doesn't\"] = 'does not'\n",
    "    reduction_dict[\"wasn't\"] = 'was not'\n",
    "    reduction_dict[\"weren't\"] = 'were not'\n",
    "    reduction_dict[\"didn't\"] = 'did not'\n",
    "    reduction_dict[\"haven't\"] = 'have not'\n",
    "    reduction_dict[\"hasn't\"] = 'has not'\n",
    "    reduction_dict[\"won't\"] = 'will not'\n",
    "    reduction_dict[\"hadn't\"] = 'had not'\n",
    "    reduction_dict[\"can't\"] = 'cannot'\n",
    "    reduction_dict[\"couldn't\"] = 'could not'\n",
    "    reduction_dict[\"mustn't\"] = 'must not'\n",
    "    reduction_dict[\"mightn't\"] = 'might not'\n",
    "    reduction_dict[\"needn't\"] = 'need not'\n",
    "    reduction_dict[\"shouldn't\"] = 'should not'\n",
    "    reduction_dict[\"oughtn't\"] = 'ought not'\n",
    "    reduction_dict[\"wouldn't\"] = 'would not'\n",
    "    reduction_dict[\"what's\"] = 'what is'\n",
    "    reduction_dict[\"how's\"] = 'how is'\n",
    "    reduction_dict[\"where's\"] = 'where is'\n",
    "\n",
    "    def uncover_reduction(string):\n",
    "        # Чтобы ускориться, заменим все символы ’ на '\n",
    "        string_tmp = string.replace('’', \"'\")\n",
    "\n",
    "        # Чтобы не перебирать все ключи для каждой строки, для начала отсеим строки, которые вообще не содержат символы сокращений\n",
    "        if (string_tmp.find(\"'\") == -1):\n",
    "            return string_tmp\n",
    "\n",
    "        # Даже если нашли, это могли быть просто слова в кавычках, проверим, что они попадают под сокращения\n",
    "        parts = (\"'s \", \"'t \", \"'ll \", \"'ve \", \"'m \", \"'d \", \"'re \")\n",
    "        founded = False\n",
    "        for part in parts:\n",
    "            if (string_tmp.find(part) != -1):\n",
    "                founded = True\n",
    "                break\n",
    "\n",
    "        # если не нашли сокращения, возвращаем строку как есть (после замены)\n",
    "        if not founded:\n",
    "            return string_tmp\n",
    "\n",
    "        # если нашли сокращения, используем наш словарь и производим замены\n",
    "        for key, value in reduction_dict.items():\n",
    "            string_tmp = string_tmp.replace(key, value)\n",
    "\n",
    "        return string_tmp\n",
    "\n",
    "    df['text_prepared'] = df['text_prepared'].apply(uncover_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:21.446806Z",
     "start_time": "2023-01-03T10:19:21.426844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_prepared</th>\n",
       "      <th>text_cleared</th>\n",
       "      <th>text_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D aww  He matches this background colour I m s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man  I m really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man try edit war guy constantly remove rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>More I can t make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "      <td>real suggestion improvement   wonder section s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation Why the edits made under my userna...      0   \n",
       "1           1  D aww  He matches this background colour I m s...      0   \n",
       "2           2  Hey man  I m really not trying to edit war  It...      0   \n",
       "3           3    More I can t make any real suggestions on im...      0   \n",
       "4           4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                       text_prepared  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i am ...   \n",
       "2  hey man, i am really not trying to edit war. i...   \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                        text_cleared  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  daww he matches this background colour i am se...   \n",
       "2  hey man i am really not trying to edit war it ...   \n",
       "3  more i cannot make any real suggestions on imp...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                         text_lemmas  \n",
       "0  explanation edit username hardcore metallica f...  \n",
       "1  daww match background colour seemingly stuck t...  \n",
       "2  hey man try edit war guy constantly remove rel...  \n",
       "3  real suggestion improvement   wonder section s...  \n",
       "4                    sir hero chance remember page s  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:22.230976Z",
     "start_time": "2023-01-03T10:19:22.185583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww  He matches this background colour I m s...</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man  I m really not trying to edit war  It...</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on im...</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Explanation Why the edits made under my userna...   \n",
       "1  D aww  He matches this background colour I m s...   \n",
       "2  Hey man  I m really not trying to edit war  It...   \n",
       "3    More I can t make any real suggestions on im...   \n",
       "4  You  sir  are my hero  Any chance you remember...   \n",
       "\n",
       "                                        text_cleared  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  daww he matches this background colour i am se...  \n",
       "2  hey man i am really not trying to edit war it ...  \n",
       "3  more i cannot make any real suggestions on imp...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Произведем очистку текста. Сохраним результат в отдельной колонке.\n",
    "\n",
    "if not USE_PREPARED_CSV:\n",
    "    def clean_text(string):\n",
    "        string = re.sub(r\"[\\n\\r]\", \" \", string)\n",
    "        string = re.sub(r\"[^a-zA-Z ]+\", \"\", string)\n",
    "        return string.strip()\n",
    "\n",
    "    df['text_cleared'] = df['text_prepared'].apply(clean_text)\n",
    "\n",
    "df[['text', 'text_cleared']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:22.991719Z",
     "start_time": "2023-01-03T10:19:22.965480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 1.43 ms, total: 11.9 ms\n",
      "Wall time: 11.2 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww  He matches this background colour I m s...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man  I m really not trying to edit war  It...</td>\n",
       "      <td>hey man try edit war guy constantly remove rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on im...</td>\n",
       "      <td>real suggestion improvement   wonder section s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Explanation Why the edits made under my userna...   \n",
       "1  D aww  He matches this background colour I m s...   \n",
       "2  Hey man  I m really not trying to edit war  It...   \n",
       "3    More I can t make any real suggestions on im...   \n",
       "4  You  sir  are my hero  Any chance you remember...   \n",
       "\n",
       "                                         text_lemmas  \n",
       "0  explanation edit username hardcore metallica f...  \n",
       "1  daww match background colour seemingly stuck t...  \n",
       "2  hey man try edit war guy constantly remove rel...  \n",
       "3  real suggestion improvement   wonder section s...  \n",
       "4                    sir hero chance remember page s  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Проведем лемматизацию\n",
    "if not USE_PREPARED_CSV:\n",
    "    lookups = Lookups()\n",
    "    lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "    #lemmatizer = Lemmatizer(lookups)\n",
    "    nlp = en_core_web_sm.load()\n",
    "\n",
    "    def lemmatize(string):\n",
    "        result = []\n",
    "        for token in nlp(string):\n",
    "            # тут же очистим стоп-слова\n",
    "            if token.is_stop == False:\n",
    "                result.append(token.lemma_)\n",
    "        return ' '.join(result)\n",
    "\n",
    "    df['text_lemmas'] = df['text_cleared'].apply(lemmatize)\n",
    "\n",
    "df[['text', 'text_lemmas']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:28.613774Z",
     "start_time": "2023-01-03T10:19:28.594977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_prepared</th>\n",
       "      <th>text_cleared</th>\n",
       "      <th>text_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D aww  He matches this background colour I m s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man  I m really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man try edit war guy constantly remove rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>More I can t make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "      <td>real suggestion improvement   wonder section s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation Why the edits made under my userna...      0   \n",
       "1           1  D aww  He matches this background colour I m s...      0   \n",
       "2           2  Hey man  I m really not trying to edit war  It...      0   \n",
       "3           3    More I can t make any real suggestions on im...      0   \n",
       "4           4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                       text_prepared  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i am ...   \n",
       "2  hey man, i am really not trying to edit war. i...   \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                        text_cleared  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  daww he matches this background colour i am se...   \n",
       "2  hey man i am really not trying to edit war it ...   \n",
       "3  more i cannot make any real suggestions on imp...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                         text_lemmas  \n",
       "0  explanation edit username hardcore metallica f...  \n",
       "1  daww match background colour seemingly stuck t...  \n",
       "2  hey man try edit war guy constantly remove rel...  \n",
       "3  real suggestion improvement   wonder section s...  \n",
       "4                    sir hero chance remember page s  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:34.597256Z",
     "start_time": "2023-01-03T10:19:29.347019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сохраним обработанный датасет для дальнейшей работы, чтобы не делать лемматизацию повторно.\n",
    "df.to_csv(PREPARED_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:53:54.994236Z",
     "start_time": "2022-07-28T13:53:54.982506Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента: </b>  Переделал с учетом обучения моделей на:  \n",
    "    \n",
    "    1. дефолтных данных  \n",
    "    \n",
    "    2. downsampling данных по классам таргета  \n",
    "    \n",
    "    3. balanced данных  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:34.722071Z",
     "start_time": "2023-01-03T10:19:34.598711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143518,), (15947,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Подготовим дефолтные признаки:\n",
    "\n",
    "#data = df[['text_lemmas','toxic']].copy()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.1, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "features_train = train['text_lemmas']\n",
    "target_train = train['toxic']\n",
    "features_test = test['text_lemmas']\n",
    "target_test = test['toxic']\n",
    "\n",
    "features_train.shape, features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:34.727951Z",
     "start_time": "2023-01-03T10:19:34.723374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128926\n",
       "1     14592\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. в дефолте будем обучать модель с дисбалансом классов:\n",
    "train['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:49.959182Z",
     "start_time": "2023-01-03T10:19:49.911142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14592\n",
       "0    14592\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вариант 2. Подготовим признаки:  проведем  downsampling для учета дисбаланса классов для train:\n",
    "train_toxic = train[train['toxic'] == 1]\n",
    "train_nontoxic = train[train['toxic'] == 0]\n",
    "\n",
    "train_downsampled = pd.concat(\n",
    "    [train_toxic] + [train_nontoxic.sample(n=len(train_toxic), random_state=RANDOM_STATE)])\n",
    "train_downsampled['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:19:52.782458Z",
     "start_time": "2023-01-03T10:19:52.775379Z"
    }
   },
   "outputs": [],
   "source": [
    "features_train_dw = train_downsampled['text_lemmas']\n",
    "target_train_dw = train_downsampled['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:20:17.323707Z",
     "start_time": "2023-01-03T10:19:55.265308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143518, 193634)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем Tf-idf\n",
    "# вариант данных №1 для дефолтной модели и disbalanced:\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=STOP_WORDS) \n",
    "count_tf_idf.fit(features_train) \n",
    "tf_idf = count_tf_idf.transform(features_train)\n",
    "print(tf_idf.shape)\n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:21:38.001668Z",
     "start_time": "2023-01-03T10:20:17.328253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143518, 2051473)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем Tf-idf #2 for ngram_range = (2,2)\n",
    " \n",
    "# вариант данных №1 для дефолтной модели и disbalanced:\n",
    "\n",
    "count_tf_idf2 = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range = (2,2)) \n",
    "count_tf_idf2.fit(features_train) \n",
    "tf_idf2 = count_tf_idf2.transform(features_train)\n",
    "print(tf_idf2.shape)\n",
    "print(tf_idf2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:21:40.804324Z",
     "start_time": "2023-01-03T10:21:38.004309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29184, 59554)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем Tf-idf\n",
    "# вариант данных №2 для  downsampling варианта датасета:\n",
    "\n",
    "count_tf_idf_dw = TfidfVectorizer(stop_words=STOP_WORDS) \n",
    "count_tf_idf_dw.fit(features_train_dw) \n",
    "tf_idf_dw = count_tf_idf_dw.transform(features_train_dw)\n",
    "print(tf_idf_dw.shape)\n",
    "print(tf_idf_dw.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:21:40.808805Z",
     "start_time": "2023-01-03T10:21:40.806571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Определим метрику оценки наших моделей согласно заданию F1:\n",
    "\n",
    "#f1 = make_scorer(f1_score, average='macro')\n",
    "f1 = make_scorer(f1_score, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### модель LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:25:57.676848Z",
     "start_time": "2023-01-03T10:21:40.810634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 3s, sys: 1min 34s, total: 9min 38s\n",
      "Wall time: 4min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.5, 'fit_intercept': True, 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Сделаем поиск лучших гиперпараметров для модели:  \n",
    "\n",
    "parameters = {'C': (0.1, 0.5, 1.0, 1.5), \n",
    "            #  \"max_iter\":(100,500),\n",
    "              \"fit_intercept\": (True, False), 'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n",
    "model = LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "grid = GridSearchCV(model, parameters, cv=3, scoring=f1)\n",
    "grid.fit(tf_idf, target_train)\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "# Для отладки - {'C': 1.0, 'fit_intercept': True, 'solver': 'saga'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:25:59.728428Z",
     "start_time": "2023-01-03T10:25:57.678200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647476901208244"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. построим для начала модель на дефолтных настройках (без учета дисбаланса классов):\n",
    "model_lr0 = LogisticRegression(random_state=RANDOM_STATE,\n",
    "                               #class_weight='balanced',\n",
    "                              fit_intercept=True, C=1.5, solver='saga')\n",
    "\n",
    "model_lr0.fit(tf_idf, target_train)\n",
    "\n",
    "tf_idf_test = count_tf_idf.transform(features_test)\n",
    "predicted_lr = model_lr0.predict(tf_idf_test)\n",
    "\n",
    "f1_score(target_test, predicted_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:04.103349Z",
     "start_time": "2023-01-03T10:25:59.729629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2551834130781499"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2. Ngrams =2.2, построим для начала модель на дефолтных настройках (без учета дисбаланса классов):\n",
    "model_lr02 = LogisticRegression(random_state=RANDOM_STATE,\n",
    "                               #class_weight='balanced',\n",
    "                              fit_intercept=True, C=1.5, solver='saga')\n",
    "\n",
    "model_lr02.fit(tf_idf2, target_train)\n",
    "\n",
    "tf_idf_test2 = count_tf_idf2.transform(features_test)\n",
    "predicted_lr02 = model_lr02.predict(tf_idf_test2)\n",
    "\n",
    "f1_score(target_test, predicted_lr02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:12.038285Z",
     "start_time": "2023-01-03T10:26:04.105200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulmatus/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7559572719802795"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. модель на дефолтных настройках + игра балансом классов (class_weight='balanced') \n",
    "model_lr = LogisticRegression(random_state=RANDOM_STATE,\n",
    "                           class_weight='balanced',\n",
    "                              fit_intercept=True, C=1.5, solver='saga')\n",
    "\n",
    "model_lr.fit(tf_idf, target_train)\n",
    "\n",
    "predicted_lr = model_lr.predict(tf_idf_test)\n",
    "\n",
    "f1_score(target_test, predicted_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:13.714759Z",
     "start_time": "2023-01-03T10:26:12.040938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.780223501523874"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. игра балансом классов вручную (class_weight={1: 1.5}) улучшило результат почти на 1.6%\n",
    "model_lr2 = LogisticRegression(random_state=RANDOM_STATE, \n",
    "                               class_weight={1: 1.5},\n",
    "                               fit_intercept=True, C=1.5, solver='saga')\n",
    "\n",
    "\n",
    "model_lr2.fit(tf_idf, target_train)\n",
    "\n",
    "predicted_lr2 = model_lr2.predict(tf_idf_test)\n",
    "\n",
    "f1_score(target_test, predicted_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:13.736781Z",
     "start_time": "2023-01-03T10:26:13.715955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14146   169]\n",
      " [  480  1152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     14315\n",
      "           1       0.87      0.71      0.78      1632\n",
      "\n",
      "    accuracy                           0.96     15947\n",
      "   macro avg       0.92      0.85      0.88     15947\n",
      "weighted avg       0.96      0.96      0.96     15947\n",
      "\n",
      "0.9593026901611589\n"
     ]
    }
   ],
   "source": [
    "# выведем confusion_matrix:  \n",
    "print(confusion_matrix(target_test, predicted_lr2))\n",
    "print(classification_report(target_test, predicted_lr2))\n",
    "print(accuracy_score(target_test, predicted_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:14.336137Z",
     "start_time": "2023-01-03T10:26:13.738636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6827852998065764"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Проверим результат на данных с downsampling признаков по классам:\n",
    "model_lr3 = LogisticRegression(random_state=RANDOM_STATE,\n",
    "                               # class_weight='balanced',\n",
    "                               fit_intercept=True, C=1.5, solver='saga')\n",
    "\n",
    "model_lr3.fit(tf_idf_dw, target_train_dw)\n",
    "tf_idf_test_dw = count_tf_idf_dw.transform(features_test)\n",
    "predicted_lr_dw = model_lr3.predict(tf_idf_test_dw)\n",
    "f1_score(target_test, predicted_lr_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:14.358476Z",
     "start_time": "2023-01-03T10:26:14.337469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13223  1092]\n",
      " [  220  1412]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95     14315\n",
      "           1       0.56      0.87      0.68      1632\n",
      "\n",
      "    accuracy                           0.92     15947\n",
      "   macro avg       0.77      0.89      0.82     15947\n",
      "weighted avg       0.94      0.92      0.93     15947\n",
      "\n",
      "0.9177274722518342\n"
     ]
    }
   ],
   "source": [
    "# выведем confusion_matrix:  \n",
    "print(confusion_matrix(target_test, predicted_lr_dw))\n",
    "print(classification_report(target_test, predicted_lr_dw))\n",
    "print(accuracy_score(target_test, predicted_lr_dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEW!!!! Промежуточные выводы:\n",
    "    Мы попробовали 4 разных варианта построения модели LogisticRegression с учетом дисбаланса классов.\n",
    "    Лучше всего результат (f1=0.78) показала модель на дефолтных данных (disbalanced) + вариант ручной настройки классов class_weight={1: 1.5} и мы даже уложились в требования ТЗ проекта.\n",
    "    хуже всего показала себя вариант предобработки данных через downsampling (f1=0.68). Странно. Т.к. на других моделях именно модели обученные на downsampling данных показали лучшие результаты. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### модель RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:30.049579Z",
     "start_time": "2023-01-03T10:26:14.360277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3643714466203411"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# модель с дефолтными данными +  (class_weight='balanced')\n",
    "classifier = RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE,\n",
    "                                    class_weight='balanced',\n",
    "                                    #class_weight={1:3},\n",
    "                                   max_depth=9)\n",
    "classifier.fit(tf_idf, target_train) \n",
    "predicted_rf = classifier.predict(tf_idf_test)\n",
    "f1_score(target_test, predicted_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:33.825091Z",
     "start_time": "2023-01-03T10:26:30.051329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40598531902879725"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Проверим результат downsampling признаков по классам: тут результат выше, чем по дефолту!\n",
    "# однако не достаточно высокий, чтобы удовлетворять требованиям ТЗ проекта.\n",
    "classifier = RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE,\n",
    "                                    #class_weight='balanced',\n",
    "                                    n_jobs = 1,\n",
    "                                    #max_samples = 20000,\n",
    "                                    #class_weight={1:3},\n",
    "                                   max_depth=9)\n",
    "classifier.fit(tf_idf_dw, target_train_dw) \n",
    "predicted_rf = classifier.predict(tf_idf_test_dw)\n",
    "f1_score(target_test, predicted_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T10:26:33.849648Z",
     "start_time": "2023-01-03T10:26:33.826363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10301  4014]\n",
      " [  194  1438]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.72      0.83     14315\n",
      "           1       0.26      0.88      0.41      1632\n",
      "\n",
      "    accuracy                           0.74     15947\n",
      "   macro avg       0.62      0.80      0.62     15947\n",
      "weighted avg       0.91      0.74      0.79     15947\n",
      "\n",
      "0.7361259171003951\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(target_test, predicted_rf))\n",
    "print(classification_report(target_test, predicted_rf))\n",
    "print(accuracy_score(target_test, predicted_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier выдал наихудшие результаты и явно не умеет работать с подобными задачами. Печаль."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### модель SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:45.994Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'C': (0.1, 0.5, 0.7, 1.0), 'kernel': ('linear', 'poly', 'rbf')}\n",
    "model_svc = SVC(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "grid = GridSearchCV(model_svc, parameters, cv=3, scoring=f1)\n",
    "grid.fit(tf_idf, target_train)\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "# Итог - {'C': 1.0, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.SVC. обучим модель с дефолтными данными +  (class_weight='balanced'):\n",
    "- почему то на этих данных ноутбук крашится (может не хватает ОЗУ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:46.590Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Обучим модель\n",
    "model_svc = SVC(random_state=RANDOM_STATE, class_weight='balanced',\n",
    "                C=1.0, kernel='linear', gamma='auto')\n",
    "model_svc.fit(tf_idf, target_train)\n",
    "\n",
    "tf_idf_test = count_tf_idf.transform(features_test)\n",
    "# Посмотрим, что получилось.\n",
    "predicted_svc = model_svc.predict(tf_idf_test)\n",
    "f1_score(target_test, predicted_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:47.009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Посмотрим, что получилось.\n",
    "predicted_svc = model_svc.predict(tf_idf_test)\n",
    "f1_score(target_test, predicted_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SVC. Проверим результат модели на данных downsampling признаков по классам: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:47.696Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Обучим модель\n",
    "model_svc2 = SVC(random_state=RANDOM_STATE, class_weight='balanced',\n",
    "                C=1.0, kernel='linear', gamma='auto')\n",
    "model_svc2.fit(tf_idf_dw, target_train_dw)\n",
    "# Посмотрим, что получилось.\n",
    "predicted_svc2 = model_svc2.predict(tf_idf_test_dw)\n",
    "f1_score(target_test, predicted_svc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:48.326Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(target_test, predicted_svc2))\n",
    "print(accuracy_score(target_test, predicted_svc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### модель CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:48.860Z"
    }
   },
   "source": [
    "1.CatBoostClassifier. обучим модель с дефолтными данными +  (class_weight='balanced'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:49.113Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pool = Pool(\n",
    "    tf_idf, target_train, \n",
    "    #cat_features=cat_features, \n",
    "    #text_features=text_features, \n",
    "    #feature_names=list(tf_idf)\n",
    ")\n",
    "    \n",
    "valid_pool = Pool(\n",
    "    tf_idf_test, \n",
    "    target_test,\n",
    "    #cat_features=cat_features, \n",
    "    #text_features=text_features, \n",
    "    #feature_names=list(tf_idf_test)\n",
    ")\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 3000,\n",
    "    'learning_rate': 0.01,\n",
    "    'eval_metric': 'F1',\n",
    "    'task_type': 'CPU',\n",
    "    'early_stopping_rounds': 300,\n",
    "    'use_best_model': True,\n",
    "    'verbose': 500}\n",
    "    \n",
    "clas_weight = [5,4]\n",
    "model_cat = CatBoostClassifier(**catboost_params,class_weights=clas_weight)\n",
    "model_cat.fit(train_pool, eval_set=valid_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:49.341Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_cat = model_cat.predict(tf_idf_test)\n",
    "f1_score(target_test, pred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:49.827Z"
    }
   },
   "source": [
    "2. CatBoostClassifier. Проверим результат модели на данных downsampling признаков по классам: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:50.060Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pool = Pool(\n",
    "    tf_idf_dw, target_train_dw, \n",
    "    #cat_features=cat_features, \n",
    "    #text_features=text_features, \n",
    "    #feature_names=list(tf_idf)\n",
    ")\n",
    "    \n",
    "valid_pool = Pool(\n",
    "    tf_idf_test_dw, \n",
    "    target_test,\n",
    "    #cat_features=cat_features, \n",
    "    #text_features=text_features, \n",
    "    #feature_names=list(tf_idf_test)\n",
    ")\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 3000,\n",
    "    'learning_rate': 0.01,\n",
    "    'eval_metric': 'F1',\n",
    "    'task_type': 'CPU',\n",
    "    'early_stopping_rounds': 300,\n",
    "    'use_best_model': True,\n",
    "    'verbose': 500}\n",
    "    \n",
    "clas_weight = [5,4]\n",
    "model_cat = CatBoostClassifier(**catboost_params,class_weights=clas_weight)\n",
    "model_cat.fit(train_pool, eval_set=valid_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:50.326Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_cat = model_cat.predict(tf_idf_test_dw)\n",
    "print(classification_report(target_test, pred_cat))\n",
    "print(accuracy_score(target_test, pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:50.574Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(target_test, pred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константная модель "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:51.771Z"
    }
   },
   "source": [
    "1.Constant-model. обучим модель с дефолтными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:52.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Проверим модель на адекватность на константной модели: f1 = 0.18\n",
    "# наши модели показывают суущественно лучший результат, и значит наш проект имел смысл!\n",
    "dummy_clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "dummy_clf.fit(tf_idf, target_train)\n",
    "predicted = dummy_clf.predict(tf_idf_test)\n",
    "f1_score(target_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:52.305Z"
    }
   },
   "source": [
    "2.Constant-model. обучим модель на данных downsampling признаков по классам: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-03T10:20:52.521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Проверим модель на адекватность на константной модели: f1 = 0.18\n",
    "# наши модели показывают суущественно лучший результат, и значит наш проект имел смысл!\n",
    "dummy_clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "dummy_clf.fit(tf_idf_dw, target_train_dw)\n",
    "predicted = dummy_clf.predict(tf_idf_test_dw)\n",
    "f1_score(target_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверили модель на адекватность на константной модели: f1 = 0.18.  \n",
    "\n",
    "Наши модели показывают суущественно лучший результат (f1 = 0.78), и значит наш проект имел смысл!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общие выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы провели анализ и обработку текстовых данных, подготовили их для двльнейшего использования в модели.  \n",
    "\n",
    "Создали несколько вариантов модели классификации.  \n",
    "\n",
    "В целом не каждая их них справилась с поставленной задачей преодолеть порог метрики в f1 = 0.75, однако все модели показали результат лучше, чем константная модель.  \n",
    "\n",
    "Однако в качестве рабочей модели мы бы предложили CatBoostClassifier или LogisticRegression, как показывающие наилучший результат (0.78)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента</b>  Спасибо за полезные ссылки и доп информацию.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
